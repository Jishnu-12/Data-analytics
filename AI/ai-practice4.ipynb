{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7bb89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = [\n",
    "    \"Averagehave Disappointed Pendrive Not supported this Projector\",\n",
    "    \"After three month use good clearity but brightness level should be improve low brightness\",\n",
    "    \"Very good product like it\",\n",
    "    \"Nice one pls go for it\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a267f3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "import string\n",
    "\n",
    "all_word =[]\n",
    "\n",
    "for i in review:\n",
    "    word = word_tokenize(i)\n",
    "    word = [w.lower() for w in word]\n",
    "    word = [w for w in word if w not in string.punctuation]\n",
    "    all_word.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ec66891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopword = set(stopwords.words('english'))\n",
    "\n",
    "clean = [\n",
    "    [w for w in h if w not in stopword]\n",
    "    for h in all_word\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52bc16dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['averagehave', 'disappointed', 'pendrive', 'supported', 'projector'], ['three', 'month', 'use', 'good', 'clearity', 'brightness', 'level', 'improve', 'low', 'brightness'], ['good', 'product', 'like'], ['nice', 'one', 'pls', 'go']]\n",
      "[['averagehav', 'disappoint', 'pendriv', 'support', 'projector'], ['three', 'month', 'use', 'good', 'cleariti', 'bright', 'level', 'improv', 'low', 'bright'], ['good', 'product', 'like'], ['nice', 'one', 'pl', 'go']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "\n",
    "lemitizer = WordNetLemmatizer()\n",
    "stem = PorterStemmer()\n",
    "\n",
    "lemmatized = [ \n",
    "    [lemitizer.lemmatize(w) for w in c]\n",
    "    for c in clean\n",
    "]\n",
    "\n",
    "stemmed = [\n",
    "    [stem.stem(w)for w in word]\n",
    "    for word in clean\n",
    "]\n",
    "\n",
    "print(lemmatized)\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f286d5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['averagehave' 'brightness' 'clearity' 'disappointed' 'go' 'good'\n",
      " 'improve' 'level' 'like' 'low' 'month' 'nice' 'one' 'pendrive' 'pls'\n",
      " 'product' 'projector' 'supported' 'three' 'use']\n",
      "[[1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0]\n",
      " [0 2 1 0 0 1 1 1 0 1 1 0 0 0 0 0 0 0 1 1]\n",
      " [0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "\n",
    "document = [' '.join(sen) for sen in lemmatized]\n",
    "\n",
    "bow = vectorizer.fit_transform(document)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a1a88f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['averagehav' 'bright' 'cleariti' 'disappoint' 'go' 'good' 'improv'\n",
      " 'level' 'like' 'low' 'month' 'nice' 'one' 'pendriv' 'pl' 'product'\n",
      " 'projector' 'support' 'three' 'use']\n",
      "[[0.4472136  0.         0.         0.4472136  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.4472136  0.         0.         0.4472136  0.4472136\n",
      "  0.         0.        ]\n",
      " [0.         0.58667444 0.29333722 0.         0.         0.23127044\n",
      "  0.29333722 0.29333722 0.         0.29333722 0.29333722 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.29333722 0.29333722]\n",
      " [0.         0.         0.         0.         0.         0.48693426\n",
      "  0.         0.         0.61761437 0.         0.         0.\n",
      "  0.         0.         0.         0.61761437 0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.5        0.\n",
      "  0.         0.         0.         0.         0.         0.5\n",
      "  0.5        0.         0.5        0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "document1 = [' '.join(sen1) for sen1 in stemmed]\n",
    "\n",
    "matrix = tfidf.fit_transform(document1)\n",
    "\n",
    "print(tfidf.get_feature_names_out())\n",
    "print(matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e72a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
