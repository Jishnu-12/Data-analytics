{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb0d412c-5502-4791-9a35-79fecc68dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I Had Enough? I am Done with The Life! I need to push pass my limits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d731eb13-d3cd-4643-bad7-79115e4ed858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'Had', 'Enough', '?', 'I', 'am', 'Done', 'with', 'The', 'Life', '!', 'I', 'need', 'to', 'push', 'pass', 'my', 'limits']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "token = word_tokenize(text)\n",
    "\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9f07e8e-ac95-41d1-a92e-282758c65fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'Had', 'Enough', 'I', 'am', 'Done', 'with', 'The', 'Life', 'I', 'need', 'to', 'push', 'pass', 'my', 'limits']\n"
     ]
    }
   ],
   "source": [
    "re = [w for w in token if w not in string.punctuation]\n",
    "\n",
    "print(re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53092b17-9358-4c06-b64a-96ebb645232f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'had', 'enough', 'i', 'am', 'done', 'with', 'the', 'life', 'i', 'need', 'to', 'push', 'pass', 'my', 'limits']\n"
     ]
    }
   ],
   "source": [
    "lower = [w.lower() for w in re]\n",
    "\n",
    "print(lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1045e82-6f17-471a-8001-269723dd528a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enough', 'done', 'life', 'need', 'push', 'pass', 'limits']\n"
     ]
    }
   ],
   "source": [
    "stopword = set(stopwords.words('english'))\n",
    "\n",
    "clean = [w for w in lower if w not in stopword]\n",
    "\n",
    "print(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2430f43c-a7e7-4b12-8124-7337ea67723a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enough', 'done', 'life', 'need', 'push', 'pas', 'limit']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "lem = [wnl.lemmatize(w) for w in clean]\n",
    "\n",
    "print(lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "482bf9a1-5a7a-41d1-98fc-b3ed9651a7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['done' 'enough' 'life' 'limit' 'need' 'pas' 'push']\n",
      "[[0.37796447 0.37796447 0.37796447 0.37796447 0.37796447 0.37796447\n",
      "  0.37796447]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "join1 = [' '.join(lem)]\n",
    "\n",
    "matrix = tfidf.fit_transform(join1)\n",
    "\n",
    "print(tfidf.get_feature_names_out())\n",
    "print(matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1028961-7361-4647-a71a-4c9b27307b77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
